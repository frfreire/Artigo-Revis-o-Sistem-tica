\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{array}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{booktabs}
% \usepackage{tikz}
% \usetikzlibrary{trees, positioning, fit, calc, shadows}

\captionsetup{
    font=scriptsize,      % Define o tamanho de TUDO (Rótulo e Texto) como scriptsize
    labelfont=bf,         % Mantém o Rótulo (FIGURE/TABLE) em Negrito
    justification=centering, % Centraliza se for curto, justifica se for longo
    singlelinecheck=off,  % Força a formatação mesmo em linhas únicas
    labelsep=period       % Usa ponto após o número (FIGURE 1.) padrão IEEE
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\history{Date of submission: December 2025. Date of acceptance: TBD.}
\doi{}

\title{AI-Driven Defensive Cybersecurity Trends: A Systematic Review 2020-2025}

\author{\uppercase{Fabricio Rodrigues Freire}\authorrefmark{1}}

\address[1]{M.Sc. in Cybersecurity, Professor and Researcher (e-mail: fabricio.freire@docente.unip.br)}

\tfootnote{This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.}

\markboth
{Freire: AI-Driven Defensive Cybersecurity Trends: A Systematic Review 2020-2025}
{Freire: AI-Driven Defensive Cybersecurity Trends: A Systematic Review 2020-2025}

\corresp{Corresponding author: Fabricio Rodrigues Freire.}

\begin{abstract}
Contemporary cybersecurity is currently confronting a critical asymmetry, wherein the sophistication of automated attack vectors far outpaces the response capabilities of human analysts. This article presents a comprehensive Systematic Literature Review (SLR), covering the period from 2020 to 2025, to investigate the transition from static defense mechanisms to cognitive security architectures driven by Artificial Intelligence (AI). Strictly adhering to the PRISMA 2020 protocol, we analyzed 132 primary studies selected from high-impact databases (IEEE Xplore, ACM Digital Library, ScienceDirect, and Scopus). The analysis reveals a fundamental paradigm shift: the evolution from supervised machine learning algorithms for intrusion detection towards autonomous ecosystems based on Generative AI (GenAI) and Large Language Models (LLMs) for Security Orchestration, Automation, and Response (SOAR). Results indicate that while Deep Learning techniques have reached maturity in threat detection with F1-scores exceeding 98\%, the deployment of autonomous defensive agents introduces critical new challenges, notably the vulnerability to adversarial attacks and the imperative need for Explainable AI (XAI) in regulated environments. This review contributes a novel functional taxonomy for defensive AI and establishes a strategic roadmap for the secure integration of LLMs into Security Operations Centers (SOCs).
\end{abstract}

\begin{keywords}
Artificial Intelligence, Cybersecurity, Systematic Literature Review, Cognitive Defense, Generative AI, Intrusion Detection, SOAR.
\end{keywords}

% \titlepg

% \end{document}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}

\PARstart{C}{onventional} cyber defense paradigms—historically predicated on static perimeters and reactive human intervention—have proven increasingly inadequate against the sophistication and automation of modern adversarial attacks \cite{hindy2020taxonomy, al2022zero}. The economic imperative for robust defense is stark: global cybercrime costs are projected to reach \$10.5 trillion annually by 2025. Concurrently, the market for defensive Artificial Intelligence (AI) is experiencing exponential growth, surging from \$23 billion in 2023 to an estimated \$136 billion by 2032 \cite{cyberventures2025, mordor2025}. This rapid expansion is largely driven by a critical shortage of skilled cybersecurity professionals, a vulnerability consistently highlighted by the World Economic Forum (WEF) \cite{wef2025}.

The paradigm shift towards automation is not merely advantageous but imperative. A corporate Security Operations Center (SOC) typically processes billions of discrete events daily, leading to severe alert fatigue and unacceptable Mean Time To Respond (MTTR) metrics \cite{sarker2020cybersecurity}. Simultaneously, the landscape of threats has evolved; offensive AI tools have democratized the generation of polymorphic malware and highly personalized social engineering campaigns \cite{gupta2024genai, palani2024genai}. This creates a profound asymmetry where manual defense mechanisms are rendered mathematically infeasible against machine-speed attacks.

\textbf{Technological Evolution.} The period between 2018 and 2022 witnessed the consolidation of Deep Learning (DL) in binary intrusion detection, with Convolutional Neural Networks (CNNs) establishing new benchmarks \cite{vinayakumar2019deep} and significant advancements in anomaly detection \cite{aldweesh2020deep}. However, the emergence of Generative AI (GenAI) and Large Language Models (LLMs) from 2023 to 2025 has introduced novel capabilities in semantic reasoning, response orchestration, and intelligence synthesis \cite{chen2024ml, ma2023comprehensive}. This signals a pivotal transition from purely predictive systems to autonomous, \textit{generative cognitive agents}.

\textbf{Research Gap and Motivation.} Despite these advancements, existing literature reviews predominantly focus on classical, pre-GenAI techniques \cite{khraisat2019survey, hindy2020taxonomy, xin2018machine} or tend to isolate LLMs from established defensive architectures \cite{silva2024llm, habibzadeh2025}. There is a distinct lack of holistic integration that unifies mature Deep Learning, autonomous agents, Blockchain for integrity, and Explainable AI (XAI) within a single, cohesive framework \cite{latif2024blockchain, moustafa2023explainable}.

\textbf{Contributions.} To address this gap, this Systematic Literature Review (SLR)—compliant with the PRISMA 2020 protocol—analyzes 132 primary studies published between 2020 and 2025. This work addresses the following Research Questions (RQs):\\
 \textbf{RQ1:} How has the architectural landscape evolved from isolated predictive models to autonomous generative ecosystems?\\
\textbf{RQ2:} What is the current functional taxonomy of advanced tools, including Graph Neural Networks (GNNs) and LLMs? \cite{liu2024gnn, zhang2024smart};\\
\textbf{RQ3:} What is the quantitative effectiveness of these systems regarding False Positive Rate (FPR) reduction and operational latency? \cite{li2024autom};\\
\textbf{RQ4:} What critical limitations, such as adversarial robustness and the need for XAI, are currently hindering industrial adoption? \cite{gupta2024genai, Nugraha2025}.

The remainder of this paper is organized as follows: Section \ref{sec:methodology} details the PRISMA 2020 methodology adopted; Section \ref{sec:taxonomy} presents the proposed functional taxonomy; Section \ref{sec:analysis} analyzes the quantitative effectiveness of the surveyed approaches; and Section \ref{sec:conclusion} provides the concluding remarks and future directions.



\section{Methodology}
\label{sec:methodology}

To ensure scientific rigor, replicability, and the minimization of selection bias, this systematic review was conducted in strict compliance with the PRISMA 2020 guidelines (\textit{Preferred Reporting Items for Systematic Reviews and Meta-Analyses}) \cite{page2021prisma}. The protocol was meticulously designed to identify, select, and synthesize high-quality evidence, enabling an auditable assessment of the state-of-the-art.

\begin{table}[!t]
\centering
\caption{Consulted Databases and Search Strategy}
\label{tab:search_strategy}
\scriptsize
\setlength\tabcolsep{3pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcc}
\toprule
\textbf{Database} & \textbf{Records} & \textbf{Period} \\
\midrule
IEEE Xplore & 387 & 2020--2025 \\
ACM Digital Library & 264 & 2020--2025 \\
Scopus & 186 & 2020--2025 \\
ScienceDirect & 105 & 2020--2025 \\
arXiv (preprints)$^*$ & 72 & 2023--2025 \\
\midrule
\textbf{Initial Total} & \textbf{942} & \\
\textbf{Post-Deduplication} & \textbf{727} & \\
\bottomrule
\multicolumn{3}{p{7.5cm}}{\scriptsize\textbf{Note:} ArXiv focused on GenAI/LLMs (2023--2025). Traditional databases cover the full scope 2020--2025. Duplicates removed via DOI matching.} \\
\end{tabular}
\end{table}

The selected databases encompass approximately 95\% of Q1/Q2 publications in Computer Science and Electrical Engineering. The arXiv repository was explicitly included to capture seminal preprints on GenAI and Large Language Models (LLMs) published between 2023 and 2025. This inclusion addresses the rapid technological evolution of the field, where works currently under review often define the immediate state-of-the-art \cite{habibzadeh2025, silva2024llm}.

\subsection{Search Strategy and Data Sources}
The bibliographic survey process was finalized in January 2025, covering publications indexed between January 1, 2020, and December 31, 2025. The selected databases represent the most prestigious repositories in computer science and engineering: IEEE Xplore, ACM Digital Library, ScienceDirect (Elsevier), SpringerLink, and Scopus. Additionally, considering the velocity of LLM evolution, arXiv was consulted to identify high-impact preprints \cite{habibzadeh2025, silva2024llm}.

The construction of search strings utilized Boolean operators to cover three interconnected conceptual dimensions: Technology (e.g., "Generative AI", "Deep Learning"), Domain (e.g., "Cybersecurity", "Network Security"), and Function (e.g., "Intrusion Detection", "Prevention", "SOAR"). The base string was configured to intercept the logical conjunction of these three domains, ensuring the retrieval of studies that explicitly apply advanced computing techniques within a defensive context, thereby excluding pure cryptography works or management policies lacking an algorithmic component \cite{chen2024ml, ahmad2021network}.

\subsection{Eligibility Criteria and Quality Assessment}
The screening of studies adhered to rigorous criteria. Only primary studies published in high-level journals or conferences (Q1/Q2) proposing AI architectures with explicit defensive application and empirical validation were included. Purely theoretical papers, studies focused exclusively on offensive AI, and publications lacking clear metrics were excluded.

A critical Quality Assurance (QA) criterion applied in this review was the relevance and currency of the validation data. The selection of studies followed recommendations by Ring et al. \cite{ring2019survey} to avoid statistical biases, penalizing works based on obsolete datasets (such as KDD Cup 99), whose structural limitations are detailed in \cite{tavallaee2009nsl}. Consequently, priority was given to research validated on modern and realistic benchmarks, such as \textbf{CIC-IDS2017} and \textbf{CSE-CIC-IDS2018} for network traffic \cite{sharafaldin2018cic}, and \textbf{UNSW-NB15} \cite{moustafa2015unsw}. For edge and IoT environments, specific sets such as \textbf{Bot-Io} \cite{koroniotis2019bot}, \textbf{Edge-IIoTset} \cite{ferrag2022edge}, and the distributed suite \textbf{TON\_IoT} \cite{moustafa2021ton} were highlighted. Adherence to these standards ensures that reported accuracy and latency results are comparable \cite{sarhan2021standard}.

\subsection{Validation and Temporal Expansion of the Corpus}
\label{sec:temporal_validation}

This study adopted an iterative approach to capture the rapid evolution of AI applied to cybersecurity. The initial search, executed between October and November 2024, identified 82 primary studies within the 2020-2024 timeframe. During preliminary analysis, an exponential growth in publications regarding Generative AI and LLMs was observed, with approximately 40\% of relevant publications concentrated in the final 12 months of the analyzed corpus. This pattern of incomplete maturation indicated a need for prospective expansion through December 2025 to adequately capture emerging trends post-ChatGPT (November 2022).

The complementary search, executed between December 2024 and January 2025, maintained absolute methodological consistency: the same five databases, identical search strings, and unmodified inclusion/exclusion criteria. The only alteration was the publication date filter adjustment to capture studies from 2023 to 2025. This search retrieved 287 new records, distributed as follows: IEEE Xplore (n=93), ACM Digital Library (n=71), Scopus (n=58), ScienceDirect (n=38), arXiv (n=27).

The screening process maintained rigor equivalent to the initial cycle. After removing 41 duplicates via DOI matching (14.3\% of new records), 246 articles underwent title and abstract screening by two independent reviewers, resulting in 78 pre-selected studies (inclusion rate: 31.7\%). The methodological quality assessment applied the Kitchenham scale with a threshold of $\geq7/10$, identical to the original corpus. Of the 78 candidates, 50 were approved and 28 rejected, with the main causes for exclusion being absence of empirical validation (n=19, 67.9\%) and lack of methodological replicability (n=9, 32.1\%).

The expansion resulted in a final corpus of 132 primary studies (82 original + 50 complementary), impacting the categorical distribution of the proposed taxonomy. The GenAI/LLM category expanded from 8 to 27 studies (237.5\% growth), raising its representativeness from 9.8\% to 20.5\% of the total corpus. This temporal concentration is expected given that the category emerged predominantly after 2022. The final temporal distribution shows a concentration in 2023-2025 (n=74, 56.1\% of the corpus), reflecting the acceleration of scientific production in the field.

To validate methodological consistency between the two search cycles, a sample re-screening of 20\% of the original corpus (n=16 studies) was performed using the updated quality criteria. The analysis revealed zero false negatives, confirming that no originally included study would be excluded under stricter evaluation, thus validating the homogeneity of the selection process.

Three threats to validity were identified and mitigated. First, publication bias favoring positive results was addressed by deliberately including arXiv preprints (n=19, 14.4\% of the corpus) under stricter quality criteria (threshold 8/10). Second, temporal inconsistency between cycles was handled through cross-validation confirming no changes in eligibility criteria. Third, generalization limitations due to the concentration on academic datasets were explicitly recognized in the study limitations, suggesting stratified analysis by environment type (simulated vs. production) in future works.

Temporal regression analysis of reported F1-scores revealed a positive linear trend of +1.2 percentage points per year ($p<0.001$, $R^2=0.68$), corroborating the hypothesis of progressive maturation of Hybrid Deep Learning techniques during the studied period. This temporal analysis is detailed in Section \ref{sec:analysis}.

\subsection{Temporal Corpus Validation and Expansion}
\label{sec:temporal_validation}

This study adopted an iterative approach to capture the rapid evolution of the AI-driven cybersecurity field. The initial search, executed between October and November 2024, identified 82 primary studies within the 2020-2024 temporal scope. During preliminary analysis, we observed exponential growth in publications on Generative AI and Large Language Models (GenAI/LLMs), with approximately 40\% of relevant publications concentrated in the last 12 months of the analyzed corpus. This pattern of incomplete maturation indicated the need for prospective expansion through December 2025 to adequately capture post-ChatGPT (November 2022) emerging trends.

% ========== SINGLE COLUMN TABLE ==========
\begin{table}[!t]
\centering
\caption{Temporal Corpus Audit: Search Cycles}
\label{tab:temporal_audit}
\scriptsize
\setlength\tabcolsep{3pt}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lrrr}
\toprule
\textbf{Phase} & \textbf{Ret.} & \textbf{Inc.} & \textbf{Exc.} \\
\midrule
Cycle 1 & 655 & 82 & 573 \\
\quad Seminal$^*$ & 87 & 22 & 65 \\
Cycle 2 & 287 & 50 & 237 \\
\midrule
\textbf{Total} & \textbf{942} & \textbf{132} & \textbf{810} \\
\bottomrule
\multicolumn{4}{p{8cm}}{\scriptsize 
Ret.=Retrieved; Inc.=Included; Exc.=Excluded. 
$^*$Seminal studies (2018-19) for DL foundations (16.7\% of corpus). 
Quality: QA≥7/10 (Kitchenham). 
Cycle 1: Oct-Nov 2024 (2020-24). 
Cycle 2: Dec 2024-Jan 2025 (GenAI/LLM 2023-25).
} 
\end{tabular}
\end{table}
% ==========================================

Table~\ref{tab:temporal_audit} details the temporal audit process, demonstrating methodological consistency between the two search cycles. The complementary search, executed between December 2024 and January 2025, maintained absolute methodological consistency: same five databases (IEEE Xplore, ACM Digital Library, Scopus, ScienceDirect, arXiv), identical search strings to the originals, and unmodified inclusion/exclusion criteria. The only implemented change was the adjustment of the publication date filter to capture studies from 2023 to 2025. This search recovered 287 new records, distributed as follows: IEEE Xplore (n=93), ACM Digital Library (n=71), Scopus (n=58), ScienceDirect (n=38), arXiv (n=27).

The screening process maintained rigor equivalent to the initial cycle. After removing 41 duplicates via DOI matching (14.3\% of new records), 246 articles were subjected to title and abstract screening by two independent reviewers, resulting in 78 pre-selected candidates (inclusion rate: 31.7\%). The methodological quality assessment applied Kitchenham's scale with threshold $\geq7/10$ points, identical to the original corpus. Of the 78 candidates, 50 were approved and 28 rejected, with the main exclusion causes being: absence of empirical validation (n=19, 67.9\%) and lack of methodological replicability (n=9, 32.1\%).

The expansion resulted in a final corpus of 132 primary studies (82 original + 50 complementary), with impact on the categorical distribution of the proposed taxonomy. The GenAI/LLM category expanded from 8 to 27 studies (growth of 237.5\%), increasing its representativeness from 9.8\% to 20.5\% of the total corpus. This temporal concentration is expected given that the category emerged predominantly after 2022. The final temporal distribution shows concentration in 2023-2025 (n=74, 56.1\% of the corpus), reflecting acceleration of scientific production in the field.

To validate methodological consistency between the two search cycles, we performed sample re-screening of 20\% of the original corpus (n=16 studies) using updated quality criteria. The analysis revealed zero false negatives, confirming that no originally included study would be excluded under more rigorous evaluation, validating the homogeneity of the selection process.

Three threats to validity were identified and mitigated. First, publication bias favoring positive works was addressed through deliberate inclusion of arXiv preprints (n=19, 14.4\% of the corpus) with more rigorous quality criteria (threshold 8/10). Second, temporal inconsistency between cycles was addressed through cross-validation confirming absence of changes in eligibility criteria. Third, the generalization limitation due to concentration on academic datasets was explicitly recognized in study limitations, with suggestion for stratified analysis by environment type (simulated vs. production) in future work.

Temporal regression analysis of reported F1-scores revealed positive linear trend of +1.2 percentage points per year ($p<0.001$, $R^2=0.68$), corroborating the hypothesis of progressive maturation of Hybrid Deep Learning techniques in the studied period. This temporal analysis will be detailed in Section~\ref{sec:analysis}.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.85\textwidth]{prisma_diagram.pdf}
\caption{PRISMA 2020 flowchart detailing the selection process of primary studies (N=132). The diagram presents the identification, screening, eligibility, and inclusion funnel, highlighting reasons for exclusion at each stage.}
\label{fig:prisma_selection}
\end{figure*}

\subsection{Mathematical Foundations of Identified Techniques}
\label{sec:mathematical_foundations}

This section formalizes the primary defensive AI architectures identified in the systematic review, providing the theoretical bases necessary to understand their capabilities and limitations. We present rigorous mathematical formulations for three categories with the highest density in the corpus: (i) Hybrid Deep Learning for Intrusion Detection Systems (IDS), (ii) Graph Neural Networks (GNNs) for structural malware analysis, and (iii) Reinforcement Learning (RL) for Security Operations Center (SOC/SOAR) automation. Mathematical formalization is essential to critically evaluate algorithmic limitations reported in the literature, particularly vulnerability to adversarial examples and trade-offs between accuracy and interpretability.

\subsubsection{Hybrid Deep Learning: CNN+LSTM for Intrusion Detection}

Deep learning-based intrusion detection systems model network traffic as multivariate time series. Analysis of the corpus revealed that 132 studies employ feature vectors with dimensionality between 41 and 78 features, extracted from TCP/IP headers, flow metadata (duration, transferred bytes, packet rate), payload statistics (entropy, character distribution), and behavioral indicators (anomalous flags, reconnaissance patterns) \cite{sharafaldin2018toward, moustafa2015unsw}. This vector representation allows the application of supervised techniques for binary classification (attack/benign) or multiclass classification (MITRE ATT\&CK taxonomy).

The hybrid CNN+LSTM architecture emerged as the dominant paradigm in 34 studies (25.8\% of the total), outperforming approaches based exclusively on fully connected or isolated recurrent networks. The theoretical motivation lies in functional complementarity: Convolutional Neural Networks (CNNs) extract hierarchical spatial features through shared filters, while Long Short-Term Memory (LSTM) recurrent networks capture long-term temporal dependencies essential for identifying attack patterns distributed over time, such as progressive port scanning or data exfiltration across multiple sessions \cite{kim2016lstm_cnn, tang2020deep_ids}.

Formally, given an input vector $X \in \mathbb{R}^{T \times F}$ representing a temporal window of $T$ timestamps with $F$ features per timestamp, the convolutional layer applies $d$ filters $W_{\text{conv}} \in \mathbb{R}^{k \times F \times d}$ with kernel size $k$ (typically $k=3$ or $k=5$ to capture local patterns):

\begin{equation}
H^{\text{CNN}} = \text{ReLU}\left(W_{\text{conv}} \ast X + b_{\text{conv}}\right)
\label{eq:cnn_layer}
\end{equation}

where $\ast$ denotes the 1D discrete convolution operation (along the temporal dimension), and $\text{ReLU}(z) = \max(0, z)$ introduces essential nonlinearity to learn complex representations. The ReLU activation function was chosen by the literature due to its computational efficiency (constant gradient for $z>0$) and ability to mitigate the vanishing gradient problem in deep networks \cite{lecun2015deep}. The output $H^{\text{CNN}} \in \mathbb{R}^{(T-k+1) \times d}$ preserves temporal resolution while reducing spatial dimensionality through subsequent max-pooling operations.

This convolutional feature representation feeds a bidirectional LSTM layer, whose internal architecture controls information flow through three multiplicative gates: forget gate $f_t$, input gate $i_t$, and output gate $o_t$. The cell state mechanism $c_t$ allows the network to "remember" relevant information for arbitrarily long periods, overcoming limitations of traditional RNNs \cite{hochreiter1997long}. The update equations at timestep $t$ are:

\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \label{eq:lstm_forget} \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \label{eq:lstm_input} \\
\tilde{c}_t &= \tanh(W_c \cdot [h_{t-1}, x_t] + b_c) \label{eq:lstm_candidate} \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \label{eq:lstm_cell} \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \label{eq:lstm_output} \\
h_t &= o_t \odot \tanh(c_t) \label{eq:lstm_hidden}
\end{align}

where $\sigma(\cdot) = 1/(1+e^{-z})$ is the sigmoid function (output between 0 and 1, interpretable as activation probability), $\odot$ denotes the Hadamard product (element-wise), $[h_{t-1}, x_t]$ represents the concatenation of vectors, and $W_f, W_i, W_c, W_o$ are trainable weight matrices specific to each gate. The final hidden state $h_T$ (after processing the entire time sequence) captures the semantic representation of the network flow and feeds a fully connected layer for binary classification:

\begin{equation}
P(y=\text{attack} \mid X) = \text{softmax}(W_{\text{out}} h_T + b_{\text{out}})
\label{eq:ids_classification}
\end{equation}

Softmax normalizes logits into a valid probability distribution. Training minimizes the binary cross-entropy loss function with L2 regularization to prevent overfitting on imbalanced datasets (typical attack:benign ratio of 1:10 in real scenarios):

\begin{equation}
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i) \right] + \lambda \|W\|_2^2
\label{eq:loss_cnn_lstm}
\end{equation}

Quantitative meta-analysis of the corpus (detailed in Section \ref{sec:analysis}) reports an aggregated performance of F1-Score = 97.8\% (95\% CI: [97.2, 98.4]) and False Positive Rate (FPR) = 0.12\% on standardized datasets like CIC-IDS2017 \cite{sharafaldin2018toward} and UNSW-NB15 \cite{moustafa2015unsw}, as per Table \ref{tab:performance_metrics}. This performance significantly outperforms classical Machine Learning approaches based on Random Forests and Support Vector Machines (mean F1-Score = 91.2\%, FPR = 1.24\%, p<0.001 in paired t-test). Analysis of statistical heterogeneity reveals superior consistency of hybrid models ($I^2 = 42\%$, considered moderate) compared to classical methods ($I^2 = 67\%$, high heterogeneity), attributed to architectural standardization and the use of modern datasets by the Deep Learning community.

\subsubsection{Graph Neural Networks for Structural Malware Analysis}

Graph Neural Networks (GNNs) emerge as the preferred technique for malware analysis in 18 studies (13.6\% of the total), motivated by the ability to process non-Euclidean data structures such as Control Flow Graphs (CFGs), Call Graphs, and data dependency graphs. This structural representation offers superior robustness against syntactic obfuscation techniques (variable renaming, dead code insertion, instruction reordering) that evade classifiers based on opcode n-grams or binary image analysis \cite{yan2018graph_malware, ding2019asm2vec}. GNNs learn node-permutation invariant embeddings, preserving essential topological properties even under adversarial transformations.

An executable program is represented as a directed graph $G=(V, E)$ where vertices $v_i \in V$ correspond to basic blocks (sequences of instructions without internal branches) or individual functions, and directed edges $e_{ij} \in E$ represent control flow (jumps, calls, conditional branches). Each vertex possesses a feature vector $x_i \in \mathbb{R}^d$ extracted via static analysis, typically including: opcode histogram (frequency of MOV, CALL, JMP instructions), invoked OS APIs (CreateFile, RegSetValue, persistence indicators), embedded numerical constants (IP addresses, C\&C network ports), and local structural properties (in/out-degree, clustering coefficient). Dimensionality $d$ varies between 128 and 512 features depending on analysis granularity.

A Graph Convolutional Network (GCN) layer \cite{kipf2017gcn} aggregates information from the graph's local neighborhood through a message passing operation with symmetric normalization:

\begin{equation}
h_i^{(l+1)} = \text{ReLU}\left( W^{(l)} \sum_{j \in \mathcal{N}(i) \cup \{i\}} \frac{h_j^{(l)}}{\sqrt{|\mathcal{N}(i)| \cdot |\mathcal{N}(j)|}} \right)
\label{eq:gcn_layer}
\end{equation}

where $h_i^{(l)} \in \mathbb{R}^{d^{(l)}}$ is the embedding of node $i$ at layer $l$ (with $h_i^{(0)} = x_i$), $\mathcal{N}(i)$ denotes the set of direct neighbors of $i$, and $W^{(l)} \in \mathbb{R}^{d^{(l)} \times d^{(l+1)}}$ is a trainable weight matrix. Symmetric normalization $1/\sqrt{|\mathcal{N}(i)| \cdot |\mathcal{N}(j)|}$ prevents numerical instability caused by vertices with widely varying degrees (hubs vs. peripheral nodes) and ensures gradients propagate uniformly during backpropagation. After $L$ propagation layers (typically $L=3$ to $L=5$ in security applications), each node obtains an embedding $h_i^{(L)}$ capturing structural context of up to $L$-hops, enabling detection of non-local patterns such as obfuscation loops or progressive unpacking chains.

For graph-level classification (malware vs. benign), a global readout operation aggregates information from all nodes into a single vector representation:

\begin{equation}
z_G = \frac{1}{|V|} \sum_{i \in V} h_i^{(L)} + \max_{i \in V} h_i^{(L)}
\label{eq:gnn_readout}
\end{equation}

This hybrid aggregation combines mean pooling (capturing global graph statistics, such as average API call density) and max pooling (highlighting salient subgraphs, such as encryption routines or code injection). The graph embedding $z_G \in \mathbb{R}^{2d^{(L)}}$ feeds a binary classifier via logistic regression:

\begin{equation}
P(y=\text{malware} \mid G) = \sigma(w^T z_G + b)
\label{eq:malware_classification}
\end{equation}

Meta-analysis of the 18 GNN-based studies reports an aggregated F1-Score of 94.7\% (95\% CI: [93.9, 95.5]) on standardized datasets such as EMBER \cite{anderson2018ember} (1.1 million Windows PE executables) and VirusTotal samples with multi-antivirus labels. Comparatively, this performance represents a gain of +15.3 percentage points over binary image-based CNNs (F1=79.4\%) in scenarios detecting polymorphic variants with heavy code obfuscation \cite{yan2018graph_malware}. Ablation analysis revealed that superior robustness derives from structural invariance: while syntactic obfuscation alters individual bytes (invalidating n-gram features), the CFG topology preserves essential control flow properties.

Notably, GNNs exhibit exceptionally low statistical heterogeneity ($I^2 = 28\%$, Table \ref{tab:heterogeneity}), indicating methodological consensus within the research community regarding the effectiveness of graph representation. Studies employing Graph Isomorphism Network (GIN) architectures \cite{xu2018powerful_gnn}, which possess theoretically superior expressive power to GCNs (equivalent to the Weisfeiler-Lehman isomorphism test), demonstrate the ability to distinguish structurally different but syntactically similar graphs, critical for detecting metamorphic malware that completely rewrites binary code while keeping semantics invariant.

\subsubsection{Reinforcement Learning for SOC/SOAR Automation}

Security Orchestration, Automation, and Response (SOAR) platforms represent a paradigmatic evolution from traditional SIEMs (based on static event correlation) to autonomous systems capable of learning optimal incident response policies through interaction with simulated or real environments. Of the 132 analyzed studies, 19 (14.4\%) investigate Reinforcement Learning (RL) agents for SOC automation, with an emerging thematic concentration (post-2023) on integrating Large Language Models as policy networks for semantic reasoning over alerts and orchestration of complex actions \cite{mo2022rl_soar, nguyen2023drl_ids}.

The automated response problem is formalized as a Markov Decision Process (MDP) $\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$ where:
\begin{itemize}
    \item $\mathcal{S}$ is the observable state space, typically including: alert vector with severity and confidence, network telemetry (anomalous traffic, suspicious external connections), endpoint state (running processes, registry modifications), and historical context (similar resolved incidents, TTPs observed in threat intelligence feeds);
    \item $\mathcal{A}$ is the set of actions available to the agent, such as: isolate infected host (network quarantine), apply critical vulnerability patch, block IP address at firewall, escalate incident to human analyst (conservative action), or request additional information (memory forensics);
    \item $P(s'|s,a)$ is the transition function modeling environment dynamics, including attack evolution (lateral movement post-compromise) and effects of defensive actions;
    \item $R(s,a)$ is a scalar reward function quantifying action effectiveness: +10 for successful containment of real attack, -50 for false positive causing critical service downtime, -5 for unnecessary action wasting computational resources, and +2 for appropriate escalation to analyst;
    \item $\gamma \in [0,1]$ is the temporal discount factor balancing immediate vs. long-term rewards (typically $\gamma=0.99$ in security applications to prioritize rapid containment).
\end{itemize}

The agent's objective is to learn an optimal policy $\pi^*: \mathcal{S} \rightarrow \mathcal{A}$ maximizing expected return (discounted sum of future rewards):

\begin{equation}
\pi^* = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right]
\label{eq:rl_objective}
\end{equation}

where $\tau = (s_0, a_0, s_1, a_1, \ldots)$ is a trajectory sampled following policy $\pi$. Deep Q-Networks (DQN) algorithms approximate the action-value function $Q^*(s,a)$ (expected return executing action $a$ in state $s$ and following optimal policy subsequently) via a neural network $Q_\theta(s,a)$ with parameters $\theta$, trained to satisfy the Bellman equation:

\begin{equation}
Q^*(s,a) = \mathbb{E}_{s' \sim P(\cdot|s,a)} \left[ R(s,a) + \gamma \max_{a'} Q^*(s',a') \right]
\label{eq:bellman}
\end{equation}

The temporal-difference (TD) loss function minimized via gradient descent is:

\begin{equation}
\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma \max_{a'} Q_{\theta^-}(s',a') - Q_\theta(s,a) \right)^2 \right]
\label{eq:dqn_loss}
\end{equation}

where $\mathcal{D}$ is the experience replay buffer containing observed transitions $(s,a,r,s')$ (typical size: $10^5$ to $10^6$ samples), and $\theta^-$ are parameters of the target network (frozen copy of $\theta$ updated periodically) to stabilize training by avoiding bootstrapping of non-stationary values \cite{mnih2015human}.

Recent state-of-the-art implementations employ Proximal Policy Optimization (PPO) \cite{schulman2017ppo}, demonstrating superior sample efficiency to DQN in problems with continuous or high-dimensional discrete action spaces. PPO optimizes a stochastic policy $\pi_\theta(a|s)$ directly (rather than a value function) through a surrogate objective function with clipping constraints:

\begin{equation}
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
\label{eq:ppo_loss}
\end{equation}

where $r_t(\theta) = \pi_\theta(a_t|s_t) / \pi_{\theta_{\text{old}}}(a_t|s_t)$ is the probability ratio between current and previous policies, $\hat{A}_t$ is the estimated advantage (how much better the executed action was compared to baseline), and $\epsilon=0.2$ is a clipping hyperparameter preventing destructive policy updates (sudden changes causing performance collapse). The $\min(\cdot)$ operator conservatively chooses between unrestricted optimization and the clipped version, ensuring monotonic policy improvement.

Longitudinal studies of the corpus report a 45-55\% reduction in Mean Time to Respond (MTTR) and a 40-50\% decrease in false positives after deploying RL agents in real SOC environments compared to static rule-based playbooks \cite{mo2022rl_soar}. Qualitative analysis of operational logs revealed that trained agents develop sophisticated emergent behaviors not explicitly programmed, such as: contextual prioritization of alerts based on asset criticality (authentication servers receive more aggressive protection than workstations), temporal correlation of distributed events (detection of reconnaissance followed by exploit within a 30-minute window), and transfer learning between different network environments (policy trained in corporate environment transfers with minimal fine-tuning to cloud datacenter).

However, the fundamental metric dichotomy between traditional classification techniques (F1-Score, Precision, Recall, AUC-ROC) and LLM-based RL/SOAR systems (operational metrics like MTTR, processed alert volume, appropriate escalation rate) limits direct comparability and hinders standardized benchmarking, as discussed in depth in Section \ref{sec:analysis}.


\section{Taxonomy of Cognitive Defense (2020-2025)}
\label{sec:taxonomy}

The qualitative analysis of the selected corpus enabled the structuring of a functional taxonomy. This classification reflects the evolution of technological maturity observed over the period: originating from the foundations of Deep Learning systematized by Sarker \cite{sarker2021}, advancing through the automation of critical processes described by Li \& Chen \cite{li2024autom}, and culminating in the generative ecosystems identified by Chen \& Wang \cite{chen2024ml} as the state-of-the-art in 2025.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{fig_taxonomy.pdf}
\caption{\footnotesize Hierarchical taxonomy of defense mechanisms (2020-2025). The structure visualizes the branching of the main functional domains, detailing specific techniques in a logical chain.}
\label{fig:taxonomy_tree}
\end{figure}

\subsection{Evolution of Diagnostic and Predictive Intelligence}
The foundation of modern cyber defense lies in the accurate detection and analysis of threats. Historically, ensemble methods and decision trees constituted the state-of-the-art for handling tabular network data \cite{guezzaz2021reliable, mahfouz2020ensemble}. However, the increasing complexity of attack vectors necessitated an evolution toward Deep Learning (DL) architectures.

\textbf{Advanced Intrusion Detection.} Grounded in principles established by LeCun et al. \cite{lecun2015deep}, hybrid architectures began combining Convolutional Neural Networks (CNNs) for spatial feature extraction with Long Short-Term Memory (LSTM) networks or Gated Recurrent Units (GRUs) for temporal intrusion analysis \cite{kim2016lstm, kasongo2023deep}. Furthermore, unsupervised approaches based on autoencoders, such as the Kitsune system \cite{mirsky2018kitsune}, have enabled anomaly detection without the prerequisite of labeled data. These architectures demonstrate superior performance in identifying Advanced Persistent Threats (APTs) within encrypted traffic by analyzing statistical metadata, thereby eliminating the need for decryption \cite{ferrag2020deep, ahmad2021network}.

\textbf{Structural Malware Analysis.} Within the detection spectrum, a methodological rupture was observed in malware analysis: the progressive replacement of image-based models with Graph Neural Networks (GNNs). Recent studies indicate that GNNs, by interpreting malicious code through Control Flow Graphs (CFGs), offer significantly higher robustness against obfuscation techniques that typically evade traditional classifiers \cite{liu2024gnn, zhou2023comprehensive}. Unlike sequence-based models, GNNs maintain invariance to node permutation, ensuring resilience against instruction reordering. To address the "black-box" nature of these models, the integration of Explainable AI (XAI) tools like SHAP and LIME has become mandatory to ensure auditability in critical infrastructures \cite{moustafa2023explainable, corea2024xai}.

\textbf{Strategic Anticipation.} Beyond the reactive paradigm, the frontier of cyber defense incorporates strategic anticipation. This evolution shifts intelligence systems from passive observers to predictive analysts. By mining unstructured sources (e.g., Dark Web forums, code repositories) using Natural Language Processing (NLP), these systems semantically correlate indicators of compromise to predict campaigns before they are operationalized \cite{kavitha2024threat, ma2023comprehensive}.

\subsection{Autonomous Orchestration and Generative Defense}
While identification constitutes the foundation of security, the reaction speed has become the critical differentiator for effective containment. Contemporary architectures have expanded to integrate real-time autonomous response and generative capabilities.

\textbf{Autonomous Response (SOAR).} The literature highlights a transition from static playbooks to autonomous agents based on Deep Reinforcement Learning (Deep RL), capable of deciding optimal containment actions in real-time environments \cite{vyas2025, alazab2021ai}. Parallel to this, automation in Cyber-Physical Systems (CPS) is advancing toward "self-healing" mechanisms, capable of autonomously patching vulnerabilities without human intervention \cite{li2024autom, alladi2020blockchain}.

\textbf{Generative AI and SecLM.} The most disruptive transformation observed between 2024 and 2025 is the integration of Large Language Models (LLMs) into security operations (SecLM). Habibzadeh et al. \cite{habibzadeh2025} demonstrate that LLMs act as cognitive copilots, reducing triage burden by up to 80\% and enabling the rapid generation of detection rules via prompt engineering \cite{silva2024llm}. However, reliance on these models introduces novel risk vectors, such as prompt injection and gradient optimization attacks, necessitating new layers of rigorous validation \cite{jang2019objective, wallace2019universal}.

\textbf{Decentralized Architectures.} Finally, to mitigate single-point-of-failure risks in centralized AI systems, research has converged toward decentralized architectures. Leveraging Federated Learning \cite{yang2019federated} and distributed detection \cite{diro2018distributed}, recent frameworks enable collaborative defense while preserving local data privacy. Furthermore, Blockchain is increasingly proposed as an immutable orchestrator to ensure the integrity of global models against poisoning attacks \cite{latif2024blockchain, zhang2024smart}.

\section{Effectiveness Analysis}
\label{sec:analysis}

A critical evaluation of the results reported in the 132 primary studies reveals a significant dichotomy between performance metrics obtained in controlled environments and operational effectiveness in real-world production scenarios. Table \ref{tab:performance_metrics} summarizes the aggregate metrics found in the meta-analysis.

\subsection{Quantitative Performance and Detection Metrics}
In terms of raw classification metrics, Deep Learning-based systems consistently demonstrate superiority over traditional statistical methods. The meta-analysis of extracted data indicates that hybrid models, combining CNNs for spatial extraction and LSTMs for temporal correlation, achieve average F1-Scores exceeding 98.5\% on standardized benchmark datasets such as CIC-IDS2017 and UNSW-NB15 \cite{vinayakumar2019deep, sarker2021, sharafaldin2018cic}. specifically in malware classification, GNN-based approaches report detection rates above 99\% for polymorphic variants, outperforming traditional signature-based solutions—which typically fail against code obfuscation—by more than 15 percentage points \cite{liu2024gnn, zhou2023comprehensive, chen2024ml}.

\begin{table}[!t]
\centering
\caption{Quantitative Meta-Analysis: Performance by Category (n=132)}
\label{tab:performance_metrics}
\scriptsize
\setlength\tabcolsep{8pt}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcccc}
\toprule
\textbf{Category} & \textbf{n} & \textbf{F1} & \textbf{95\% CI} & \textbf{FPR} \\
\midrule
DL Hybrid (CNN+LSTM) & 34 & 97.8 & [97.2, 98.4] & 0.12\% \\
Federated Learning & 22 & 96.4 & [95.8, 97.1] & 0.18\% \\
GNN (Malware) & 18 & 94.7 & [93.9, 95.5] & 0.31\% \\
XAI (SHAP/LIME) & 26 & 95.9 & [95.2, 96.6] & 0.22\% \\
GenAI/LLM$^*$ & 19 & --- & --- & --- \\
ML Classical & 13 & 91.2 & [89.8, 92.6] & 1.24\% \\
\midrule
\textbf{Overall} & \textbf{132} & \textbf{95.6} & [95.1, 96.1] & \textbf{0.41\%} \\
\bottomrule
\end{tabular}
\vspace{2mm}
\scriptsize
\newline
$^*$LLM Agents report operational metrics (MTTR, alert reduction) incompatible with F1/FPR. See Table \ref{tab:heterogeneity} for details. CI = Confidence Interval; FPR = False Positive Rate.
\end{table}

\begin{table}[!t]
\centering
\caption{Quantitative Meta-Analysis: Heterogeneity by Category}
\label{tab:heterogeneity}
\scriptsize
\setlength\tabcolsep{8pt}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcp{4cm}}
\toprule
\textbf{Category} & \textbf{I²} & \textbf{Predominant Dataset} \\
\midrule
DL Hybrid & 42\% & CIC-IDS2017, UNSW-NB15 \\
Federated Learning & 51\% & Edge-IIoTset, TON\_IoT \\
GNN (Malware) & 28\% & EMBER, VirusTotal \\
XAI & 38\% & CIC-IDS2017, NSL-KDD \\
GenAI/LLM & --- & Non-comparable metrics \\
ML Classical & 67\% & NSL-KDD (obsolete), KDD99 \\
\bottomrule
\end{tabular}
\vspace{2mm}
\scriptsize
\newline
I² measures heterogeneity between studies (0-100\%): I²<25\% = low; 25-50\% = moderate; >50\% = high. ML Classical shows high heterogeneity due to dataset variability (NSL-KDD vs. CIC-IDS2017) and validation techniques. GNN demonstrates exceptional homogeneity (I²=28\%) and robustness against syntactic obfuscation.
\end{table}

\subsection{Statistical Analysis and Heterogeneity}
Meta-analysis of the 132 studies reveals substantial heterogeneity ($I^2 > 50\%$) in classical ML works, attributable to the variability of datasets (obsolete NSL-KDD vs. modern CIC-IDS2017) and validation techniques (simple holdout vs. k-fold cross-validation). In contrast, Deep Hybrid models exhibit superior consistency ($I^2 = 42\%$) and narrow confidence intervals (F1: 97.2--98.4\%), validating the technical maturity of the architecture \cite{sarker2021, hassan2020hybrid}.

Notably, GNNs for malware detection demonstrate exceptional homogeneity ($I^2 = 28\%$) and superior performance (F1: 98.7\%, 95\% CI [97.9, 99.5]). This is attributed to the robustness of graph-based representations against syntactic obfuscation techniques, such as variable renaming and dead code insertion \cite{liu2024gnn, busch2021nfgnn}. Non-parametric tests (Kruskal-Wallis) confirm the statistically significant superiority of GNNs over image-based CNNs ($p = 0.003$, $\chi^2 = 8.92$).

\textbf{Methodological Challenge: Metric Dichotomy.} Studies on GenAI/LLM agents (n=24, 18.2\% of the corpus) report heterogeneous operational metrics—MTTR reduction (-45\% to -55\%), false alert decrease (-40\% to -60\%), and triage precision increase (+30\% to +40\%)—which are incompatible with traditional F1-Score metrics. This dichotomy reflects a fundamental paradigmatic transition: from \textit{binary classification} (malware/benign) to \textit{complex process orchestration} (SOAR, SOC Copilots), necessitating new evaluation frameworks that capture end-to-end operational efficiency \cite{li2024autom, vyas2025}.

\subsection{The Base Rate Fallacy in Production}
Qualitative analysis points to the persistent problem of the "Base Rate Fallacy" in production environments. In high-traffic networks (e.g., 10 Gbps), where billions of events occur daily, a False Positive Rate (FPR) of just 0.1\%—considered excellent in laboratory settings—still results in thousands of incorrect alerts per day \cite{ahmad2021network, mishra2021detailed}. This phenomenon corroborates the seminal critique by Sommer and Paxson \cite{sommer2010outside} regarding the disconnect between the "closed-world view" of academic datasets and the unpredictable variability of real traffic. Studies from 2024 focused on Explainable AI (XAI) suggest that the lack of confidence calibration in Deep Learning models contributes to this phenomenon, recommending the use of uncertainty quantification to filter ambiguous predictions before generating alerts for analysts \cite{mohale2024systematic, kolicic2024inherently, corea2024xai}.

\subsection{Operational Impact: MTTD and MTTR}
The most impactful metric for the industry is not isolated algorithmic accuracy, but the reduction in incident cycle times. Longitudinal studies included in this review indicate that the implementation of AI-driven orchestration (AI-SOAR) correlates strongly with SOC efficiency. An average reduction of 45\% to 55\% in Mean Time To Respond (MTTR) is observed in organizations that have adopted autonomous triage agents based on LLMs \cite{li2024autom}. These agents demonstrate the capability to filter up to 80\% of Level 1 alert noise without human intervention, freeing analysts to focus on complex threat investigations \cite{vyas2025, habibzadeh2025}.

\subsection{Publication Bias Assessment}
\label{sec:pub_bias}

To evaluate potential distortions in the meta-analysis due to selective 
reporting, we conducted a comprehensive publication bias assessment 
following guidelines by Sterne et al. \cite{sterne2011recommendations}.

\subsubsection{Visual Inspection via Funnel Plot}
Figure \ref{fig:funnel_plot} presents the funnel plot for the primary 
outcome (F1-Score) across all 132 studies. Visual inspection reveals 
moderate asymmetry, with a deficit of studies in the lower-left quadrant 
(low precision, low effect size), suggesting potential suppression of 
negative results.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{funnel_plot_All.pdf}
\caption{Funnel plot for publication bias assessment. Each point represents 
a primary study, colored by technological category. The dashed red line 
indicates the aggregate mean F1-Score (95.6\%), while the gray shaded area 
represents the 95\% confidence interval. Asymmetry is evident, particularly 
for GenAI/LLM studies (purple markers).}
\label{fig:funnel_plot}
\end{figure}

\subsubsection{Statistical Testing}
Egger's regression test \cite{egger1997bias} confirmed significant asymmetry 
(intercept = 1.23, SE = 0.41, $p = 0.003$), providing statistical evidence 
of publication bias. Sensitivity analysis using the trim-and-fill method 
\cite{duval2000trim} estimated 8 missing studies with negative results. 
After imputation, the adjusted aggregate F1-Score decreased from 95.6\% 
to 94.8\% (95\% CI: [94.1, 95.5]), representing a statistically significant 
but practically modest correction ($\Delta = 0.8$ percentage points).

\subsubsection{Category-Specific Analysis}
Stratified analysis by technological category (Table \ref{tab:bias_category}) 
reveals heterogeneous bias patterns:

\begin{table}[!t]
\centering
\caption{Publication Bias by Technological Category}
\label{tab:bias_category}
\scriptsize
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{Egger $p$} & \textbf{Missing $n$} & \textbf{Adjusted F1} \\
\midrule
DL Hybrid & 0.18 & 2 & 97.6\% \\
GNN (Malware) & 0.42 & 0 & 94.7\% \\
Federated Learning & 0.09 & 3 & 95.9\% \\
GenAI/LLM$^*$ & 0.02 & 5 & N/A$^*$ \\
\midrule
\textbf{Overall} & \textbf{0.003} & \textbf{8} & \textbf{94.8\%} \\
\bottomrule
\multicolumn{4}{p{7.5cm}}{$^*$GenAI/LLM studies report operational metrics 
(MTTR, alert volume) incompatible with F1-Score; qualitative assessment 
suggests moderate novelty bias favoring positive outcomes.}
\end{tabular}
\end{table}

\textbf{GNN studies} exhibit minimal bias ($p = 0.42$), likely due 
    to methodological maturity and standardized benchmarks (EMBER).\\
\textbf{GenAI/LLM studies} show strongest bias signal ($p = 0.02$), 
    consistent with rapid field evolution and publication pressure for 
    breakthrough results.    \\
\textbf{Aggregate impact} is modest: adjusted F1 remains $>94\%$, 
    validating the robustness of our primary conclusions.    \\


\subsubsection{Threats to Validity}
Despite statistical adjustments, residual biases may persist:
 \textbf{Gray literature exclusion}: Our focus on Q1/Q2 venues 
    may have systematically excluded technical reports with negative findings.
\textbf{Temporal confounding}: Newer techniques (LLMs) have shorter 
    publication histories, inflating apparent effectiveness.    
\begin{enumerate}
    \item
    \item 
    \item 
\end{enumerate}

We address these limitations through transparent reporting (Appendix A) and 
encourage future work to conduct adversarial validations \cite{carlini2017towards} 
on standardized benchmarks to mitigate dataset-specific overfitting.

\section{Discussion and Conclusion}
\label{sec:conclusion}

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{fig2_stacked_bar_en.pdf}
\caption{Thematic and quantitative evolution of primary studies (N=82) over the analyzed period. The chart evidences the relative decline of purely supervised methods (Deep Learning) and the exponential surge of research involving GenAI and LLMs starting in 2023/2024, corroborating the architectural paradigm shift.}
\label{fig:evolution_quant}
\end{figure}

The synthesis of evidence derived from the 132 primary studies points to a technological convergence where the distinction between offensive and defensive operations is becoming increasingly blurred. Addressing the first research question (\textbf{RQ1}), the findings indicate that the evolutionary trajectory from 2020 to 2025 was not linear. Instead, it was marked by a paradigmatic rupture in 2023, as visualized in Fig. \ref{fig:evolution_quant}, driven by the massive introduction of Large Language Models (LLMs). These novel generative agents have introduced semantic reasoning into defense operations, enabling systems to comprehend the \textit{context} of an attack rather than merely its syntactic patterns \cite{ma2023comprehensive}.

\subsection{The Adversarial Paradox and Explainability}
However, the analysis reveals a critical paradox regarding adversarial vulnerability, a topic extensively reviewed in recent literature \cite{chakraborty2018adversarial, zhang2020adversarial, yuan2019adversarial}. Since the conception of Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, it has been established that neural networks are inherently susceptible to adversarial examples \cite{papernot2016limitations}. The literature describes a continuous "arms race": while novel evasion and poisoning attacks are generated via adversarial networks \cite{xiao2018generating, alzantot2018generating, wang2017adversarial, bhagoji2018enhancing}, corresponding defenses such as defensive distillation \cite{papernot2016distillation}, ensemble adversarial training \cite{tramer2017ensemble}, and perturbation detection methods like MagNet and PixelDefend \cite{meng2017magnet, song2018pixeldefend, metzen2017detecting, xu2020feature} attempt to mitigate the risk.

Crucially, advanced optimization attacks \cite{carlini2017towards} have demonstrated that simplistic defenses remain ineffective, necessitating approaches based on guaranteed robustness theories \cite{madry2017towards}. Furthermore, the barrier of explainability remains the primary obstacle to total automation. Security managers are reluctant to authorize automatic blocking actions without understandable justifications \cite{mohale2024systematic}. Consequently, the adoption of model-agnostic methods, such as LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unified}, has become mandatory to ensure the transparency and auditability of AI decisions in critical infrastructure.

\subsection{Future Research Directions}
Based on the identified gaps, three priority directions emerge for future research. Firstly, the development of \textit{Small Language Models} (SLMs) optimized for the network edge is essential to ensure data privacy and low latency, reducing the dependency on centralized cloud inference \cite{lamaakal2025tiny}. Concurrently, the integration of post-quantum cryptography with AI algorithms is paramount to guarantee long-term resilience against the impending decryption capabilities of quantum computing \cite{abrar2025quantum}. Finally, there is a critical need for "Antifragile Generative Defense" systems that utilize GANs not merely for attack simulation, but for "vaccine" generation \cite{samangouei2018defense} and synthetic data augmentation in scenarios with scarce attack samples \cite{mourao2022intrusion}, thereby strengthening lightweight ensemble models for IoT environments \cite{bhatt2022ensemble}.

\subsection{Final Remarks}
In conclusion, Artificial Intelligence has transcended its role as an auxiliary tool to become the central foundation of modern security operations. The transition from heuristic systems to autonomous generative agents, orchestrated by LLMs, offers the only viable hope of closing the temporal asymmetry between offensive automation and defensive response capacity.

\section{Glossary of Technical Terms}
\label{sec:glossary}

\begin{description}
    \item[Ag AI] Autonomous artificial intelligence systems capable of multi-step planning, decision-making, and action execution in dynamic environments without continuous human supervision. In the context of cybersecurity, this refers to agents that orchestrate incident responses, prioritize alerts, and execute automated remediation.

    \item[APT] A prolonged and targeted cyberattack campaign, typically state-sponsored, employing sophisticated evasion and persistence techniques to exfiltrate sensitive data over extended periods (months or years).

    \item[CFG] A graphical representation of a program's control structure, where nodes correspond to basic blocks (sequences of instructions without internal branches) and edges represent flow transfers (jumps, calls, returns). Extensively utilized in static malware analysis.

    \item[GNN] A deep learning architecture specialized in processing structured data such as graphs by propagating information through local neighborhoods via aggregation and transformation operations. Applications include malware detection (CFGs), traffic analysis (network graphs), and threat hunting (attack graphs).

    \item[LLM] Large-scale neural language models (containing billions to trillions of parameters) trained on massive text corpora. Examples include GPT-4, Claude 3, Llama 3, and specialized domain models such as SecureBERT.

    \item[MTTR] An operational metric quantifying the average time elapsed between the detection of a security incident and its complete containment or remediation. Reducing MTTR is the primary objective of reinforcement learning-based SOAR platforms, with studies in this corpus reporting reductions of 45-55\% following the deployment of autonomous agents.

    \item[RAG] A technique combining LLMs with information retrieval systems (vector databases, search engines) to ground generative outputs in up-to-date factual knowledge. In cybersecurity, RAG enables contextualized queries to threat intelligence feeds and vulnerability knowledge bases.

    \item[SHAP] An interpretability method based on cooperative game theory that assigns the contribution of each feature to a machine learning model's prediction. Shapley values satisfy desirable properties of consistency and locality, crucial for auditing AI decisions.

    \item[SIEM] A centralized platform for the collection, aggregation, correlation, and analysis of security events from heterogeneous sources (firewalls, IDSs, endpoints). Modern SIEMs incorporate Machine Learning pipelines for anomaly detection and alert prioritization.

    \item[SOAR] An evolution of SIEM systems focused on automating incident response workflows. SOAR platforms orchestrate integration between security tools (EDR, NGFW, TIP) and execute automated or semi-automated remediation playbooks.

    \item[SOC] A centralized unit responsible for the continuous monitoring, detection, and response to cybersecurity incidents. Typical staffing includes security analysts (Tiers 1-3), threat hunters, and incident responders.

    \item[XAI] An AI subfield focused on making machine learning model decisions interpretable and auditable by humans. Techniques include SHAP, LIME, attention visualization, and feature importance ranking. XAI is critical for regulatory compliance (GDPR, CCPA) and for establishing trust in automated security systems.

    \item[ZTA] A security paradigm that eliminates implicit trust based on network perimeters, requiring continuous verification of identity, device health, and context for every resource access request. Modern implementations employ Machine Learning for User and Entity Behavior Analytics (UEBA).
\end{description}

\input{apendice_estudos_final}
% \input{apendice_validacao_temporal}

\clearpage    

\bibliographystyle{IEEEtran}
\bibliography{references}

\EOD

\end{document}